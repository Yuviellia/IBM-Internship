{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/IBM/watson-machine-learning-samples/raw/master/cloud/notebooks/headers/AutoAI-Banner_Pipeline-Notebook.png)\n",
    "# Pipeline 4 Notebook - AutoAI Notebook v1.15.0\n",
    "\n",
    "Consider these tips for working with an auto-generated notebook:\n",
    "- Notebook code generated using AutoAI will execute successfully. If you modify the notebook, we cannot guarantee it will run successfully.\n",
    "- This pipeline is optimized for the original data set. The pipeline might fail or produce sub-optimum results if used with different data.  If you want to use a different data set, consider retraining the AutoAI experiment to generate a new pipeline. For more information, see <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/autoai-notebook.html\">Cloud Platform</a> \n",
    "- Before modifying the pipeline or trying to re-fit the pipeline, consider that the code converts dataframes to numpy arrays before fitting the pipeline (a current restriction of the preprocessor pipeline).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"content\"></a>\n",
    "## Notebook content\n",
    "\n",
    "This notebook contains a Scikit-learn representation of AutoAI pipeline. This notebook introduces commands for getting data, training the model, and testing the model. \n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.7 and scikit-learn 0.23.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Notebook goals\n",
    "\n",
    "-  Scikit-learn pipeline definition\n",
    "-  Pipeline training \n",
    "-  Pipeline evaluation\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "**[Setup](#setup)**<br>\n",
    "&nbsp;&nbsp;[Package installation](#install)<br>\n",
    "&nbsp;&nbsp;[AutoAI experiment metadata](#variables_definition)<br>\n",
    "**[Pipeline inspection](#inspection)** <br>\n",
    "&nbsp;&nbsp;[Read training data](#read)<br>\n",
    "&nbsp;&nbsp;[Train and test data split](#split)<br>\n",
    "&nbsp;&nbsp;[Make pipeline](#preview_model_to_python_code)<br>\n",
    "&nbsp;&nbsp;[Train pipeline model](#train)<br>\n",
    "&nbsp;&nbsp;[Test pipeline model](#test_model)<br>\n",
    "**[Next steps](#next_steps)**<br>\n",
    "**[Copyrights](#copyrights)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\"></a>\n",
    "## Package installation\n",
    "Before you use the sample code in this notebook, install the following packages:\n",
    " - ibm_watson_machine_learning,\n",
    " - autoai-libs,\n",
    " - scikit-learn,\n",
    " - xgboost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:00:45.009458Z",
     "iopub.status.busy": "2020-10-12T14:00:45.007968Z",
     "iopub.status.idle": "2020-10-12T14:00:46.037702Z",
     "shell.execute_reply": "2020-10-12T14:00:46.038270Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ibm-watson-machine-learning\n",
    "!pip install -U autoai-libs\n",
    "!pip install -U scikit-learn==0.23.2\n",
    "!pip install -U xgboost==1.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"variables_definition\"></a>\n",
    "## AutoAI experiment metadata\n",
    "The following cell contains the training data connection details.  \n",
    "**Note**: The connection might contain authorization credentials, so be careful when sharing the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:00:49.797633Z",
     "iopub.status.busy": "2020-10-12T14:00:49.796778Z",
     "iopub.status.idle": "2020-10-12T14:00:57.182715Z",
     "shell.execute_reply": "2020-10-12T14:00:57.183132Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#@hidden_cell\n",
    "from ibm_watson_machine_learning.helpers import DataConnection\n",
    "from ibm_watson_machine_learning.helpers import S3Connection, S3Location\n",
    "\n",
    "training_data_reference = [DataConnection(\n",
    "    connection=S3Connection(\n",
    "        api_key='PZeV9yeiXNsxL0cIhsuERsaqjUwryg4m_UkT0ZbHttXw',\n",
    "        auth_endpoint='https://iam.bluemix.net/oidc/token/',\n",
    "        endpoint_url='https://s3-api.us-geo.objectstorage.softlayer.net'\n",
    "    ),\n",
    "        location=S3Location(\n",
    "        bucket='internshiptest-donotdelete-pr-lbx9gwuhgc1cis',\n",
    "        path='credit_risk_training.csv'\n",
    "    )),\n",
    "]\n",
    "training_result_reference = DataConnection(\n",
    "    connection=S3Connection(\n",
    "        api_key='PZeV9yeiXNsxL0cIhsuERsaqjUwryg4m_UkT0ZbHttXw',\n",
    "        auth_endpoint='https://iam.bluemix.net/oidc/token/',\n",
    "        endpoint_url='https://s3-api.us-geo.objectstorage.softlayer.net'\n",
    "    ),\n",
    "    location=S3Location(\n",
    "        bucket='internshiptest-donotdelete-pr-lbx9gwuhgc1cis',\n",
    "        path='auto_ml/2d1ad8db-0082-4482-afb3-513c7992d64d/wml_data/2493a106-a7b0-4878-9b3d-db5924f77c74/data/automl',\n",
    "        model_location='auto_ml/2d1ad8db-0082-4482-afb3-513c7992d64d/wml_data/2493a106-a7b0-4878-9b3d-db5924f77c74/data/automl/hpo_c_output/Pipeline1/model.pickle',\n",
    "        training_status='auto_ml/2d1ad8db-0082-4482-afb3-513c7992d64d/wml_data/2493a106-a7b0-4878-9b3d-db5924f77c74/training-status.json'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell contains input parameters provided to run the AutoAI experiment in Watson Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:00:57.187305Z",
     "iopub.status.busy": "2020-10-12T14:00:57.186602Z",
     "iopub.status.idle": "2020-10-12T14:00:57.188392Z",
     "shell.execute_reply": "2020-10-12T14:00:57.188878Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment_metadata = dict(\n",
    "   prediction_type='classification',\n",
    "   prediction_column='Risk',\n",
    "   holdout_size=0.1,\n",
    "   scoring='accuracy',\n",
    "   deployment_url='https://us-south.ml.cloud.ibm.com',\n",
    "   csv_separator=',',\n",
    "   random_state=33,\n",
    "   max_number_of_estimators=2,\n",
    "   training_data_reference=training_data_reference,\n",
    "   training_result_reference=training_result_reference,\n",
    "   project_id='1560bdad-b82c-4f14-b062-016783069295',\n",
    "   positive_label='Risk',\n",
    "   drop_duplicates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inspection\"></a>\n",
    "# Pipeline inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"read\"></a>\n",
    "## Read training data\n",
    "\n",
    "Retrieve training dataset from AutoAI experiment as pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:01:16.076169Z",
     "iopub.status.busy": "2020-10-12T14:01:16.075589Z",
     "iopub.status.idle": "2020-10-12T14:01:19.190233Z",
     "shell.execute_reply": "2020-10-12T14:01:19.190807Z"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = training_data_reference[0].read(csv_separator=experiment_metadata['csv_separator'])\n",
    "df.dropna('rows', how='any', subset=[experiment_metadata['prediction_column']], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split\"></a>\n",
    "##  Train and test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "X = df.drop([experiment_metadata['prediction_column']], axis=1).values\n",
    "y = df[experiment_metadata['prediction_column']].values\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=experiment_metadata['holdout_size'],\n",
    "                                                    stratify=y, random_state=experiment_metadata['random_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"preview_model_to_python_code\"></a>\n",
    "## Make pipeline\n",
    "In the next cell, you can find the Scikit-learn definition of the selected AutoAI pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from autoai_libs.transformers.exportable import NumpyColumnSelector\n",
    "from autoai_libs.transformers.exportable import CompressStrings\n",
    "from autoai_libs.transformers.exportable import NumpyReplaceMissingValues\n",
    "from autoai_libs.transformers.exportable import NumpyReplaceUnknownValues\n",
    "from autoai_libs.transformers.exportable import boolean2float\n",
    "from autoai_libs.transformers.exportable import CatImputer\n",
    "from autoai_libs.transformers.exportable import CatEncoder\n",
    "import numpy as np\n",
    "from autoai_libs.transformers.exportable import float32_transform\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from autoai_libs.transformers.exportable import FloatStr2Float\n",
    "from autoai_libs.transformers.exportable import NumImputer\n",
    "from autoai_libs.transformers.exportable import OptStandardScaler\n",
    "from sklearn.pipeline import make_union\n",
    "from autoai_libs.transformers.exportable import NumpyPermuteArray\n",
    "from autoai_libs.cognito.transforms.transform_utils import TA2\n",
    "import autoai_libs.utils.fc_methods\n",
    "from autoai_libs.cognito.transforms.transform_utils import FS1\n",
    "from autoai_libs.cognito.transforms.transform_utils import TAM\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#import for Pipeline constructor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#import for feature seleciton\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#import for GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#imports for creating confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#imports for scorers\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing & Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numpy_column_selector_0 = NumpyColumnSelector(\n",
    "    columns=[\n",
    "        0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
    "    ]\n",
    ")\n",
    "compress_strings = CompressStrings(\n",
    "    compress_type=\"hash\",\n",
    "    dtypes_list=[\n",
    "        \"char_str\", \"int_num\", \"char_str\", \"char_str\", \"char_str\", \"char_str\",\n",
    "        \"int_num\", \"char_str\", \"char_str\", \"int_num\", \"char_str\", \"int_num\",\n",
    "        \"char_str\", \"char_str\", \"int_num\", \"char_str\", \"int_num\", \"char_str\",\n",
    "        \"char_str\",\n",
    "    ],\n",
    "    missing_values_reference_list=[\"\", \"-\", \"?\", float(\"nan\")],\n",
    "    misslist_list=[\n",
    "        [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [],\n",
    "        [], [],\n",
    "    ],\n",
    ")\n",
    "numpy_replace_missing_values_0 = NumpyReplaceMissingValues(\n",
    "    missing_values=[], filling_values=float(\"nan\")\n",
    ")\n",
    "numpy_replace_unknown_values = NumpyReplaceUnknownValues(\n",
    "    filling_values=float(\"nan\"),\n",
    "    filling_values_list=[\n",
    "        float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"),\n",
    "        float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"),\n",
    "        float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"),\n",
    "        float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\"),\n",
    "    ],\n",
    "    missing_values_reference_list=[\"\", \"-\", \"?\", float(\"nan\")],\n",
    ")\n",
    "cat_imputer = CatImputer(\n",
    "    strategy=\"most_frequent\",\n",
    "    missing_values=float(\"nan\"),\n",
    "    sklearn_version_family=\"23\",\n",
    ")\n",
    "cat_encoder = CatEncoder(\n",
    "    encoding=\"ordinal\",\n",
    "    categories=\"auto\",\n",
    "    dtype=np.float64,\n",
    "    handle_unknown=\"error\",\n",
    "    sklearn_version_family=\"23\",\n",
    ")\n",
    "pipeline_0 = make_pipeline(\n",
    "    numpy_column_selector_0,\n",
    "    compress_strings,\n",
    "    numpy_replace_missing_values_0,\n",
    "    numpy_replace_unknown_values,\n",
    "    boolean2float(),\n",
    "    cat_imputer,\n",
    "    cat_encoder,\n",
    "    float32_transform(),\n",
    ")\n",
    "numpy_column_selector_1 = NumpyColumnSelector(columns=[4])\n",
    "float_str2_float = FloatStr2Float(\n",
    "    dtypes_list=[\"int_num\"], missing_values_reference_list=[]\n",
    ")\n",
    "numpy_replace_missing_values_1 = NumpyReplaceMissingValues(\n",
    "    missing_values=[], filling_values=float(\"nan\")\n",
    ")\n",
    "num_imputer = NumImputer(strategy=\"median\", missing_values=float(\"nan\"))\n",
    "opt_standard_scaler = OptStandardScaler(\n",
    "    num_scaler_copy=None,\n",
    "    num_scaler_with_mean=None,\n",
    "    num_scaler_with_std=None,\n",
    "    use_scaler_flag=False,\n",
    ")\n",
    "pipeline_1 = make_pipeline(\n",
    "    numpy_column_selector_1,\n",
    "    float_str2_float,\n",
    "    numpy_replace_missing_values_1,\n",
    "    num_imputer,\n",
    "    opt_standard_scaler,\n",
    "    float32_transform(),\n",
    ")\n",
    "union = make_union(pipeline_0, pipeline_1)\n",
    "numpy_permute_array = NumpyPermuteArray(\n",
    "    axis=0,\n",
    "    permutation_indices=[\n",
    "        0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 4,\n",
    "    ],\n",
    ")\n",
    "ta2 = TA2(\n",
    "    fun=np.multiply,\n",
    "    name=\"product\",\n",
    "    datatypes1=[\n",
    "        \"intc\", \"intp\", \"int_\", \"uint8\", \"uint16\", \"uint32\", \"uint64\", \"int8\",\n",
    "        \"int16\", \"int32\", \"int64\", \"short\", \"long\", \"longlong\", \"float16\",\n",
    "        \"float32\", \"float64\",\n",
    "    ],\n",
    "    feat_constraints1=[autoai_libs.utils.fc_methods.is_not_categorical],\n",
    "    datatypes2=[\n",
    "        \"intc\", \"intp\", \"int_\", \"uint8\", \"uint16\", \"uint32\", \"uint64\", \"int8\",\n",
    "        \"int16\", \"int32\", \"int64\", \"short\", \"long\", \"longlong\", \"float16\",\n",
    "        \"float32\", \"float64\",\n",
    "    ],\n",
    "    feat_constraints2=[autoai_libs.utils.fc_methods.is_not_categorical],\n",
    "    col_names=[\n",
    "        \"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\",\n",
    "        \"LoanAmount\", \"ExistingSavings\", \"EmploymentDuration\",\n",
    "        \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\",\n",
    "        \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"InstallmentPlans\",\n",
    "        \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\", \"Telephone\",\n",
    "        \"ForeignWorker\",\n",
    "    ],\n",
    "    col_dtypes=[\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "    ],\n",
    ")\n",
    "fs1_0 = FS1(\n",
    "    cols_ids_must_keep=range(0, 20),\n",
    "    additional_col_count_to_keep=20,\n",
    "    ptype=\"classification\",\n",
    ")\n",
    "tam = TAM(\n",
    "    tans_class=FeatureAgglomeration(),\n",
    "    name=\"featureagglomeration\",\n",
    "    col_names=[\n",
    "        \"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\",\n",
    "        \"LoanAmount\", \"ExistingSavings\", \"EmploymentDuration\",\n",
    "        \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\",\n",
    "        \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"InstallmentPlans\",\n",
    "        \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\", \"Telephone\",\n",
    "        \"ForeignWorker\", \"product(LoanDuration__LoanAmount)\",\n",
    "        \"product(LoanDuration__Age)\", \"product(LoanAmount__Age)\",\n",
    "    ],\n",
    "    col_dtypes=[\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "        np.dtype(\"float32\"), np.dtype(\"float32\"),\n",
    "    ],\n",
    ")\n",
    "fs1_1 = FS1(\n",
    "    cols_ids_must_keep=range(0, 20),\n",
    "    additional_col_count_to_keep=20,\n",
    "    ptype=\"classification\",\n",
    ")\n",
    "xgb_classifier = XGBClassifier(\n",
    "    base_score=0.5,\n",
    "    booster=\"gbtree\",\n",
    "    colsample_bylevel=1,\n",
    "    colsample_bynode=1,\n",
    "    colsample_bytree=1.0,\n",
    "    gamma=0,\n",
    "    gpu_id=-1,\n",
    "    interaction_constraints=\"\",\n",
    "    learning_rate=1.0,\n",
    "    max_delta_step=0,\n",
    "    max_depth=1,\n",
    "    min_child_weight=1,\n",
    "    monotone_constraints=\"()\",\n",
    "    n_estimators=50,\n",
    "    n_jobs=2,\n",
    "    num_parallel_tree=1,\n",
    "    random_state=33,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=1.0,\n",
    "    scale_pos_weight=1,\n",
    "    subsample=0.8,\n",
    "    tree_method=\"hist\",\n",
    "    validate_parameters=1,\n",
    "    verbosity=0,\n",
    "    nthread=2,\n",
    "    silent=True,\n",
    "    seed=33,\n",
    ")\n",
    "selection = SelectFromModel(xgb_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define scoring metod\n",
    "Choose scoring strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default value\n",
    "#scoring_method = accuracy_score\n",
    "\n",
    "#Second case\n",
    "scoring_method = f1_score\n",
    "\n",
    "#Third case\n",
    "#scoring_method = precision_score\n",
    "\n",
    "#Foruth case\n",
    "#scoring_method = recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', union), \n",
    "    ('numpy_permute_array', numpy_permute_array), \n",
    "    ('ta2', ta2),\n",
    "    ('fs1_0', fs1_0),\n",
    "    ('tam', tam),\n",
    "    ('fs1_1', fs1_1),\n",
    "    ('selection', selection),\n",
    "    ('model', xgb_classifier),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_depth': [1, 2, 3, 4, 5, 7, 10],\n",
    "    'model__learning_rate': [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n",
    "    'model__n_estimators': [x for x in range(50, 60)],\n",
    "    'model__min_child_weight': [1, 5, 10],\n",
    "    'model__gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'model__subsample': [0.6, 0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.6, 0.8, 1.0],   \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, scoring=make_scorer(scoring_method, average='binary', pos_label='Risk'))\n",
    "\n",
    "grid.fit(train_X, train_y)\n",
    "print(f\"Best parameters: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## Train pipeline model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scorer\n",
    "This cell constructs the scorer based on scoring method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def print_score(method):\n",
    "    return {\n",
    "        accuracy_score: accuracy_score(test_y, pipeline.predict(test_X)),\n",
    "        f1_score: f1_score(test_y, pipeline.predict(test_X), average='binary', pos_label='Risk'),\n",
    "        recall_score: recall_score(test_y, pipeline.predict(test_X), average='binary', pos_label='Risk'),\n",
    "        precision_score: precision_score(test_y, pipeline.predict(test_X), average='binary', pos_label='Risk'),\n",
    "    }[method]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"test_model\"></a>\n",
    "### Fit pipeline model\n",
    "In this cell, the pipeline is fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:01:19.291734Z",
     "iopub.status.busy": "2020-10-12T14:01:19.244735Z",
     "iopub.status.idle": "2020-10-12T14:01:19.338461Z",
     "shell.execute_reply": "2020-10-12T14:01:19.338958Z"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test_model\"></a>\n",
    "## Test pipeline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score the fitted pipeline with the chosen scorer using the holdout dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:02:03.910267Z",
     "iopub.status.busy": "2020-10-12T14:02:03.909710Z",
     "iopub.status.idle": "2020-10-12T14:02:03.914154Z",
     "shell.execute_reply": "2020-10-12T14:02:03.914727Z"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Result: \", print_score(scoring_method), \"%\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(pipeline, test_X, test_y, cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy corectness percent for risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_y, pipeline.predict(test_X), labels = ['No Risk', 'Risk'])\n",
    "print(\"Result: \", cm[1][1] / (cm[1][0] + cm[1][1]), \"%\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"next_steps\"></a>\n",
    "# Next steps\n",
    "\n",
    "#### [Model deployment as webservice](https://github.com/IBM/watson-machine-learning-samples/tree/master/cloud/notebooks/python_sdk/deployments/autoai)\n",
    "#### [Run AutoAI experiment with python SDK](https://github.com/IBM/watson-machine-learning-samples/tree/master/cloud/notebooks/python_sdk/experiments/autoai)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"copyrights\"></a>\n",
    "### Copyrights\n",
    "\n",
    "Licensed Materials - Copyright © 2021 IBM. This notebook and its source code are released under the terms of the ILAN License.\n",
    "Use, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n",
    "\n",
    "**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs  \n",
    "(or equivalent) and License Information document for Watson Studio Auto-generated Notebook (License Terms),  \n",
    "such agreements located in the link below. Specifically, the Source Components and Sample Materials clause  \n",
    "included in the License Information document for Watson Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n",
    "\n",
    "By downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"http://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\">License Terms</a>\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
